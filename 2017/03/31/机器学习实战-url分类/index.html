<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 机器学习实战(url分类) · Tech blog of Unicooo</title><meta name="description" content="机器学习实战(url分类) - Unicooo"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="http://fonts.useso.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><div id="mask" style="display: none;"><img id="mask-image" src="#" style=" "></div><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/Windsooon" target="_blank" class="nav-list-link">GITHUB</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">机器学习实战(url分类)</h1><div class="post-time">Mar 31, 2017</div><div class="post-content"><p>实际应用中，我们经常需要对集合进行分类，一方面可以进行统计，另一方面也可以用作去重。常见的例如<strong>url分类</strong>（本质上是字符串分类，不过又因为url的特殊性使其需要特别处理），<strong>地址分类</strong>（经纬度分类），<strong>文章分类</strong>（文章分类我们之后会介绍）。分类的其中一种思路是先把集合量化为几项特征值，使其可以计算出个体间的距离。然后通过分类算法（这里使用的是k-means算法）进行分类。</p>
<p>  这里举的例子是url分类，我们从爬虫获取到url，需要把它们分成不同的类别。我们先分析一个url：</p>
<pre><code>https://www.example.com/abc/123?id=1 
</code></pre><p>我们可以把url抽象成几个特征：</p>
<p><strong>http还是https，域名，目录，目录深度，参数，参数数量，域名在Alexa上的排名。域名在搜索引擎的收录数</strong></p>
<p>这一步非常重要，有些开发者往往只在乎算法的效率和实现，忽略了从需求角度去对数据进行分析和提取。就像理查德费曼所说的，“无论计算机多少神奇，你给它的是垃圾，它出来的也是垃圾”。</p>
<p>分析以上的url我们可以得到：</p>
<ol>
<li>访问安全性：https</li>
<li>域名为：www.example.com</li>
<li>目录：abc/123</li>
<li>目录深度：三级</li>
<li>参数：id=1</li>
<li>参数数量：1个</li>
<li>Alexa排名：10000（非真实）</li>
<li>搜索引擎收录数：1000（非真实）</li>
</ol>
<p>我们根据需求给这8个特征赋予不同的权重</p>
<pre><code>Similarity = 域名 * 0.4 + 目录 * 0.2 + 目录深度 * 0.2 + 参数 * 0.1 + 参数数量 * 0.1 
</code></pre><p>在这里我们只使用5个特征，特征的数量对具体实现并没有影响。不过这里需要注意一个问题，在计算距离的时候我们可以使用<strong>欧式距离（Euclidean Distance）</strong>，<strong>编辑距离（Edit Distance）</strong>，<strong>余弦相似性（Cosine Similarity）</strong>。欧式距离公式如下：</p>
<p>在这里我们可以看到，不同的特征由于它的绝对值范围不同（例如域名一般会在20个字符串内，而Alexa排名却可以在一到几百万的区间），对最终距离影响的范围也不同。例如在所有url中Alexa排名最前的拍在第100位，最后为2000位。那么这两个url的距离因为排名这个特征被过度放大了。所以在计算距离之前，我们常常需要归一化数值，将特征值的取值范围局限在0到1或者-1到1之间，公式如下：</p>
<pre><code>newValue = (oldValue - min) / (max - min)
</code></pre><p>假如集合中一个url的排名为500，那么在计算过之后就变成</p>
<pre><code>newValue = (500 - 100) / (2000 - 100) = 0.21
</code></pre><p>之后可以计算url之间每个特征的编辑距离。</p>
<pre><code>https://www.example.com/abc/123?id=1
https://www.example.com/abc/cdf/258?id=19
</code></pre><p>这里的编辑距离分别为，0，6，1，1，0，（这是归一化数值之前的编辑距离，实际应用需要先获得每个特征值的距离最大最小值，之后再把编辑距离转化为0到1之间）计算编辑距离的代码如下：</p>
<pre><code>import numpy as np

def levenshtein(source, target):
    &apos;&apos;&apos; 使用矩阵计算比其他方法快40% &apos;&apos;&apos;
    if len(source) = len(target).
    if len(target) == 0:
        return len(source)

    # We call tuple() to force strings to be used as sequences
    # (&apos;c&apos;, &apos;a&apos;, &apos;t&apos;, &apos;s&apos;) - numpy uses them as values by default.
    source = np.array(tuple(source))
    target = np.array(tuple(target))

    # We use a dynamic programming algorithm, but with the
    # added optimization that we only need the last two rows
    # of the matrix.
    previous_row = np.arange(target.size + 1)
    for s in source:
        # Insertion (target grows longer than source):
        current_row = previous_row + 1

        # Substitution or matching:
        # Target and source items are aligned, and either
        # are different (cost of 1), or are the same (cost of 0).
        current_row[1:] = np.minimum(
                current_row[1:],
                np.add(previous_row[:-1], target != s))

        # Deletion (target grows shorter than source):
        current_row[1:] = np.minimum(
                current_row[1:],
                current_row[0:-1] + 1)

        previous_row = current_row

    return previous_row[-1]
</code></pre><p>其他实现方法可以参考<a href="https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python" target="_blank" rel="external">编辑距离</a></p>
<p>知道怎么计算距离，就可以使用分类算法，简单的可以使用k-means。这里为了讲解，并不是用numpy库</p>
<pre><code>import random
import functools
from ml_logging import setup_logging


setup_logging(__name__)


def read_from_file(file_name, dimension_num, split_char):
    &apos;&apos;&apos;从文件读取数据
       file_name: 数据源文件名
       dimension_num: 选择的数据维度数量
       split_char: 数据分隔符
    &apos;&apos;&apos;
    def list_to_float(
            line, dimension_num=dimension_num, split_char=split_char):
        l = [float(x) for x in line.replace(&quot;\n&quot;, &quot;&quot;).split(
            split_char)[:dimension_num]]
        return l

    try:
        with open(file_name, &apos;r&apos;) as data:
            # 若每行数据源为a1 a2 a3
            # 返回格式为float数值
            # [[a1, a2, a3], [b1, b2, b3], [c1, c2, c3]]
            dataset = [list_to_float(x) for x in data]
    except IOError:
        pass
    return dataset


def cluster_random(dataset, chuster_num):
    &apos;&apos;&apos;随机选择质点
       dataset: 数据源
       chuster_num: 质点数量
    &apos;&apos;&apos;
    return random.sample(dataset, chuster_num)


def cal_distance(a, b):
    &apos;&apos;&apos;计算两个数据源之间的欧几里得距离
       dataset: 数据源
       chuster_num: 质点数量
    &apos;&apos;&apos;
    return functools.reduce(
        lambda x, y: x+y, [(x - y) ** 2 for x, y in zip(a, b)]) ** 0.5


def cal_multi(cluster_list, dataset):
    &apos;&apos;&apos;把每一个点分到最近的质心，返回分组后的情况
       cluster_list: 质心列表
       dataset: 数据源
    &apos;&apos;&apos;
    di = {}
    for i in dataset:
        ans = list(map(lambda x: cal_distance(i, x), cluster_list))
        cluster_index = ans.index(min(ans))
        di.setdefault(cluster_index, []).append(i)
        # di返回的格式应该如下：
        # di = {
        #          1: [[a1, a2], [c1, c2]],
        #          2: [[b1, b2], [d1, d2]]
        #      }
    return di


def new_cluster(cluster_dict):
    &apos;&apos;&apos;获取新的聚簇点
       cluster_dict: 包含分组信息的字典
    &apos;&apos;&apos;
    def average(nums):
        return sum(nums)/len(nums)

    return list(
        map(lambda x: (average(n) for n in zip(*x.values())), cluster_dict))


def main(file_name, dimension_num, split_char, k):
    # 读取数据
    dataset = read_from_file(file_name, dimension_num, split_char)
    # 选择初始的k个质点
    cluster_list = cluster_random(dataset, k)
    # 这是第一次分组
    cluster_before = False
    # 默认分组为空
    cluster_dict = {}
    while 1:
        if cluster_before:
            # 根据现有分组情况计算新的质心
            new_cluster(cluster_dict)
        else:
            cur_dict = cal_multi(cluster_list, dataset)
            cluster_before = True
            if cur_dict == cluster_dict:
                break
            else:
                cluster_dict = cur_dict
</code></pre><p>使用numpy实现可以参考<a href="https://datasciencelab.wordpress.com/2013/12/12/clustering-with-k-means-in-python/" target="_blank" rel="external">python的k-means实现</a></p>
<p>这里使用k-means算法，太受随机起始点的影响，可以结合二分k-means和k-means++进行分类，使分类更加稳定。</p>
</div></article></div></section><footer><div class="paginator"><a href="/2017/03/31/What Bayes'theorem tell us/" class="prev">PREV</a><a href="/2017/03/31/docker基础/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'windsonYang';
var disqus_identifier = '2017/03/31/机器学习实战-url分类/';
var disqus_title = '机器学习实战(url分类)';
var disqus_url = 'https://windsooon.github.io/2017/03/31/机器学习实战-url分类/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//windsonYang.disqus.com/count.js" async></script><div class="copyright"><p>© 2016 - 2017 <a href="https://windsooon.github.io">Unicooo</a>, unless otherwise noted.</p></div></footer></div><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-65933410-1",'auto');ga('send','pageview');</script><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script></body></html>